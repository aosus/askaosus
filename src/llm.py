import logging
from typing import List

from openai import OpenAI, AsyncOpenAI

from .config import Config
from .discourse import DiscoursePost

logger = logging.getLogger(__name__)


class LLMClient:
    """Handles communication with Language Learning Models."""
    
    def __init__(self, config: Config):
        """Initialize the LLM client."""
        self.config = config
        
        # Initialize OpenAI-compatible client
        client_kwargs = config.get_openai_client_kwargs()
        self.client = AsyncOpenAI(**client_kwargs)
        
        # Load system prompt
        self.system_prompt = self._load_system_prompt()
    
    def _load_system_prompt(self) -> str:
        """Load the system prompt from file."""
        try:
            with open("/app/system_prompt.md", "r", encoding="utf-8") as f:
                return f.read().strip()
        except FileNotFoundError:
            # Fallback system prompt
            return self._get_default_system_prompt()
    
    def _get_default_system_prompt(self) -> str:
        """Get the default system prompt."""
        return """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù„Ù…Ø¬ØªÙ…Ø¹ Ø£Ø³Ø³ØŒ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ Ø§Ù„Ø¹Ø±Ø¨ÙŠ Ø§Ù„Ø£ÙƒØ¨Ø± Ù„Ù„Ø¨Ø±Ù…Ø¬ÙŠØ§Øª Ø§Ù„Ø­Ø±Ø© ÙˆØ§Ù„Ù…ÙØªÙˆØ­Ø© Ø§Ù„Ù…ØµØ¯Ø±.

Ù…Ù‡Ù…ØªÙƒ Ù‡ÙŠ Ù…Ø³Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ† Ø¨Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø£Ø³Ø¦Ù„ØªÙ‡Ù… Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹. 

Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:
1. Ø§ÙƒØªØ¨ Ø¥Ø¬Ø§Ø¨Ø© Ù‚ØµÙŠØ±Ø© ÙˆÙ…ÙÙŠØ¯Ø© Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
2. Ø§Ø³ØªÙ†Ø¯ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù‚Ø¯Ù… Ù…Ù† Ø§Ù„Ù…Ø¬ØªÙ…Ø¹
3. Ø§Ø°ÙƒØ± Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø© Ù„Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„
4. Ø´Ø¬Ø¹ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¹Ù„Ù‰ Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ Ù„Ù„Ø§Ø³ØªÙØ§Ø¯Ø© Ø£ÙƒØ«Ø±
5. Ø¥Ø°Ø§ Ù„Ù… ØªØ¬Ø¯ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø©ØŒ Ø§Ø¹ØªØ°Ø± ÙˆÙˆØ¬Ù‡ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø·Ø±Ø­ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙÙŠ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹

Ø§Ø­Ø±Øµ Ø¹Ù„Ù‰ Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø®ØªØµØ±Ø© (Ø£Ù‚Ù„ Ù…Ù† 500 ÙƒÙ„Ù…Ø©) ÙˆÙ…Ø´Ø¬Ø¹Ø© Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ø²ÙŠØ§Ø±Ø© Ø§Ù„Ù…Ø¬ØªÙ…Ø¹."""
    
    async def generate_answer(self, question: str, search_results: List[DiscoursePost]) -> str:
        """
        Generate an answer to a question based on search results.
        
        Args:
            question: The user's question
            search_results: List of relevant Discourse posts
            
        Returns:
            Generated answer
        """
        try:
            # Log the search results first
            logger.info(f"Processing question: {question}")
            logger.info(f"Found {len(search_results)} search results from Discourse")
            
            for i, result in enumerate(search_results):
                logger.debug(f"Result {i+1}: {result.title} - {result.url}")
                logger.debug(f"  Excerpt: {result.excerpt[:200]}...")
            
            # Prepare context from search results
            context = self._prepare_context(search_results)
            logger.debug(f"Prepared context length: {len(context)} characters")
            
            # Prepare messages for the LLM
            user_message = f"""
Ø§Ù„Ø³Ø¤Ø§Ù„: {question}

Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ù…Ø¬ØªÙ…Ø¹ Ø£Ø³Ø³:
{context}

Ø£Ø¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù‚Ø¯Ù…. Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø®ØªØµØ±Ø© ÙˆÙ…ÙÙŠØ¯Ø©ØŒ ÙˆØ§Ø°ÙƒØ± Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù†Ø´ÙˆØ± Ø§Ù„Ø£ÙƒØ«Ø± ØµÙ„Ø©.
"""
            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": user_message}
            ]
            # Log the LLM input messages
            logger.info("LLM input messages:")
            for msg in messages:
                logger.info(f"  {msg['role']}: {msg['content']}")
            
            # Log the input to LLM
            logger.info(f"Sending request to LLM provider: {self.config.llm_provider}")
            logger.info(f"Model: {self.config.llm_model}")
            logger.debug(f"System prompt length: {len(self.system_prompt)} characters")
            logger.debug(f"User message length: {len(user_message)} characters")
            
            # Generate response
            response = await self.client.chat.completions.create(
                model=self.config.llm_model,
                messages=messages,
                max_tokens=self.config.llm_max_tokens,
                temperature=self.config.llm_temperature,
            )
            
            # Log the response
            answer = response.choices[0].message.content.strip()
            logger.info(f"LLM response received - length: {len(answer)} characters")
            # Log LLM output
            logger.info(f"LLM output: {answer}")
            logger.debug(f"Raw LLM response: {answer}")
            
            # Log usage if available
            if hasattr(response, 'usage') and response.usage:
                logger.info(f"Token usage - prompt: {response.usage.prompt_tokens}, completion: {response.usage.completion_tokens}, total: {response.usage.total_tokens}")
            
            # Add a footer encouraging forum participation
            answer += f"\n\nðŸ’¬ Ù„Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ù†Ø§Ù‚Ø´Ø§Øª ÙˆØ§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©ØŒ ØªÙØ¶Ù„ Ø¨Ø²ÙŠØ§Ø±Ø© Ù…Ø¬ØªÙ…Ø¹ Ø£Ø³Ø³: {self.config.discourse_base_url}"
            
            return answer
            
        except Exception as e:
            logger.error(f"Error generating answer: {e}", exc_info=True)
            return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø£Ùˆ Ø²ÙŠØ§Ø±Ø© Ù…Ø¬ØªÙ…Ø¹ Ø£Ø³Ø³ Ù…Ø¨Ø§Ø´Ø±Ø©: https://discourse.aosus.org"
    
    def _prepare_context(self, search_results: List[DiscoursePost]) -> str:
        """Prepare context string from search results."""
        if not search_results:
            return "Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ø¨Ø­Ø«"
        
        context_parts = []
        for i, post in enumerate(search_results):
            context_parts.append(f"""
Ø§Ù„Ù…Ù†Ø´ÙˆØ± {i+1}:
Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: {post.title}
Ø§Ù„Ù…Ø­ØªÙˆÙ‰: {post.excerpt}
Ø§Ù„Ø±Ø§Ø¨Ø·: {post.url}
Ø¹Ø¯Ø¯ Ø§Ù„Ø¥Ø¹Ø¬Ø§Ø¨Ø§Øª: {post.like_count}
Ø¹Ø¯Ø¯ Ø§Ù„Ø±Ø¯ÙˆØ¯: {post.reply_count}
---""")
        
        return "\n".join(context_parts)
